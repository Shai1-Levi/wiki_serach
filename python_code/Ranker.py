import csv
import pandas as pd
class Ranker:
  def __init__(self, path):
    self.page_rank_df = pd.read_csv(path)

  def read_page_rank(self, path):
     self.page_rank_df = pd.read_csv(path)

  def get_page_rank_by_ids(self, page_ids):
      relevence = self.page_rank_df[self.page_rank_df['id'].isin(page_ids)]
      outpot_lst = [-1]*len(page_ids)
      for index, row in relevence.iterrows():
          outpot_lst[page_ids.index(row['id'])] = row['rank']
      for i in range(len(outpot_lst)):
        if outpot_lst[i] == -1:
          outpot_lst[i]=0
      return outpot_lst



# -*- coding: utf-8 -*-
"""assignment_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HG0oZ_a7S3yR1UA2GC_Lg1RgPjlRrbTp

# Assignment 4: Ranking & Evaluation

## General guidelines

This notebook contains considerable amount of code to help you complete this assignment. Your task is to implement any missing parts of the code and answer any questions (if exist) within this notebook. This will require understanding the existing code, may require reading about packages being used, reading additional resources, and maybe even going over your notes from class ðŸ˜±

**Evaluation and auto-grading**: Your submissions will be evaluated using both automatic and manual grading. Code parts for implementation are marked with a comment `# YOUR CODE HERE`, and usually followed by cell(s) containing automatic tests that evaluate the correctness of your answer. Additionaly, staff will manually assess your submission. Any automatic tests that did not run due to your notebook timing out **will automatically receive 0 points**. The execution time excludes initial data download, which will already exist in the testing environment. The staff reserves the right to **modify any grade provided by the auto-grader** as well as to **execute additional tests not provided to you**. It is also important to note that **auto-graded cells only result in full or no credit**. In other words, you must pass all tests implemented in a test cell in order to get the credit for it, and passing some, but not all, of the tests in a test cell will not earn you any points for that cell. 

**Submission**: Unless specified otherwise, you need to upload this notebook file **with your ID as the file name**, e.g. 012345678.ipynb, to the assignment on Moodle.

# Tasks

In this assignment, we are going to build a complete retrieval system using the inverted index (assignment 2)

**Your tasks in this assignment are:**

1. (25 Points) Implement ranking using TF-IDF and BM25.
2. (20 Points) Implement search/retrieval using the inverted index. Implement compound ranking based on title and body. 
4. (15 Points) Using weighting of title and body scores. Provide three examples of mistakes that the model is making and explanations for why, and describe how you will change the model based on these observations.
5. (25 Points) Implement the multiple metrics, including precision, recall, NDCG, MRR, and more.
6. (5 Points) Optimize your ranking function for MAP@100 using the training set. Validate that the final submitted model passes the test of MAP@100 on the test set. 
7. (10 Points) Plot graphs. First, plot metrics scores for the different queries. Second, plot metrics scores with varying values of k. (e.g., MRR@1, MRR@3, MRR@5, etc.')

# Imports
"""
import wget
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from collections import defaultdict,Counter
import re
import nltk
import pickle
import numpy as np
nltk.download('stopwords')

from nltk.corpus import stopwords
from tqdm import tqdm
import operator
from itertools import islice,count
from contextlib import closing

import json
from io import StringIO
from pathlib import Path
from operator import itemgetter
import pickle
import matplotlib.pyplot as plt

wget_download = wget.download("http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz")
# !wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz
# !tar -xf cran.tar.gz

"""## Tasks 1: Implement ranking using TF-IDF and BM25 (25 points).
In this task, you need to implement TF-IDF and BM25. At this point, for simplicity, we do not use an inverted index. However, later on in this assignment, we will.

Implementation remarks:
* TF-IDF: use [sklearn TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). To deal with stopwords use the argument `stop_words`. (5 points)

TfidfVectorizer suggests handling with additional parameters, as you can read in the documentation. Make sure you read about them.

* Cosine Similarity: use [sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html). (5 points)

* BM25: Implement a BM25 version according to the provided skeleton **without the use of packages**. 10 points as follows:
    * Prepare the data and filter stopwords. (5 points)
    * Implement two functions at BM25 class. (5 points)

* Ranking: implement topN functionallity (5 points)

**Later in this assignment, we will write code for TF-IDF and BM25 that utilize the inverted index.**
"""

# set of documents
data = ['The sky is blue and we can see the blue sun.',
        'The sun is bright and yellow.',
        'here comes the blue sun',
        'Lucy in the sky with diamonds and you can see the sun in the sky',
        'sun sun blue sun here we come',
        'Lucy likes blue bright diamonds']

"""### TF-IDF

**YOUR TASK (5 POINTS):**  Complete the implementation of `tf_idf_scores`, which calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.
"""

def tf_idf_scores(data):
    """
    This function calculates the tfidf for each word in a single document utilizing TfidfVectorizer via sklearn.

    Parameters:
    -----------
      data: list of strings.
    
    Returns:
    --------
      Two objects as follows:
                                a) DataFrame, documents as rows (i.e., 0,1,2,3, etc'), terms as columns ('bird','bright', etc').
                                b) TfidfVectorizer object.

    """
    # YOUR CODE HERE
    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(data)
    lst = vectorizer.get_feature_names_out()
    df = pd.DataFrame(X.todense(), columns=lst)
    return df, vectorizer
    
df_tfidfvect, tfidfvectorizer = tf_idf_scores(data)

#tests
assert df_tfidfvect.shape[1] == 10 and df_tfidfvect.shape[0] == 6
assert 'is' not in df_tfidfvect.columns and 'we' not in df_tfidfvect.columns
assert 'sun' in df_tfidfvect.columns and 'yellow' in df_tfidfvect.columns
assert round(df_tfidfvect.max(),3).max() == 0.798
assert np.count_nonzero(df_tfidfvect) == 21
assert type(tfidfvectorizer) == TfidfVectorizer

"""Now, upon existing TF-IDF matrix we are seeking for the similarity for given new queries. 
First we need to convert the **new** queries into a vector format. Therefore we use transform instead of fit_transform.

Next, we would like to calculate the cosine similarity between queires and documents.


"""

queries = ['look the the blue sky', 'He likes the blue the sun','Lucy likes blue sky with diamonds']
queries_vector = tfidfvectorizer.transform(queries)

"""Now, lets calculate the cosine similarity utilizing sklearn.

**YOUR TASK (5 POINTS):**  Complete the implementation of `cosine_sim_using_sklearn`.
You need to compute the similarity between the queries and the given documents.
"""

def cosine_sim_using_sklearn(queries,tfidf):
    """
    In this function you need to utilize the cosine_similarity function from sklearn.
    You need to compute the similarity between the queries and the given documents.
    This function will return a DataFrame in the following shape: (# of queries, # of documents).
    Each value in the DataFrame will represent the cosine_similarity between given query and document.
    
    Parameters:
    -----------
      queries: sparse matrix represent the queries after transformation of tfidfvectorizer.
      documents: sparse matrix represent the documents.
      
    Returns:
    --------
      DataFrame: This function will return a DataFrame in the following shape: (# of queries, # of documents).
      Each value in the DataFrame will represent the cosine_similarity between given query and document.
    """
    # YOUR CODE HERE
    df_queries = pd.DataFrame(queries.todense())
    return cosine_similarity(df_queries, tfidf, dense_output=True)

cosine_sim_df = cosine_sim_using_sklearn(queries_vector,df_tfidfvect)

# tests for cosine similarity
assert cosine_sim_df.shape[0] == len(queries)
assert cosine_sim_df.shape[1] == len(data)
assert (abs(cosine_sim_df)>1).any().any() == False
assert np.count_nonzero(cosine_sim_df) == 16
assert round(cosine_sim_df.max(),3).max() == 0.888

"""### BM25
In this section we will create a class of Best Match 25 (BM25)

To use BM25 we will need to following parameters:

* $k1$ and $b$ - free parameters
* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$
* |$D$|- is the length of the document $D$ in terms 
* $avgdl$ -  average document length
* $IDF$ - which is calculted as follows: $ln(\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df)).

As a reminder of the data looks like this
"""

data

"""**Note:** While using TF-IDF from sklearn we used the `stop_words` argument. 

**It is not the case when working with BM25 without any package.**
Therefore, we need to filter the data and clean it. We will do so utilizing NLTK stopwords
"""

RE_WORD = re.compile(r"""[\#\@\w](['\-]?\w){2,24}""", re.UNICODE)
stopwords_frozen = frozenset(stopwords.words('english'))
def tokenize(text):
    """
    This function aims in tokenize a text into a list of tokens. Moreover, it filter stopwords.
    
    Parameters:
    -----------
    text: string , represting the text to tokenize.    
    
    Returns:
    -----------
    list of tokens (e.g., list of tokens).
    """
    list_of_tokens =  [token.group() for token in RE_WORD.finditer(text.lower()) if token.group() not in stopwords_frozen]    
    return list_of_tokens


clean_data = [tokenize(doc) for doc in data]

"""Now let's find all the parameters needed for our toy example.
**Later on we will need to gather this information from the inverted index.**

**YOUR TASK (5 POINTS):**  Complete the implementation of `bm25_preprocess`.
This function goes through the data and saves relevant information for the calculation of bm25.
"""

def bm25_preprocess(data):
    """
    This function goes through the data and saves relevant information for the calculation of bm25. 
    Specifically, in this function, we will create 3 objects that gather information regarding document length, term frequency and
    document frequency.
    Parameters
    -----------
    data: list of lists. Each inner list is a list of tokens. 
    Example of data: 
    [
        ['sky', 'blue', 'see', 'blue', 'sun'],
        ['sun', 'bright', 'yellow'],
        ['comes', 'blue', 'sun'],
        ['lucy', 'sky', 'diamonds', 'see', 'sun', 'sky'],
        ['sun', 'sun', 'blue', 'sun'],
        ['lucy', 'likes', 'blue', 'bright', 'diamonds']
    ]
    
    Returns:
    -----------
    three objects as follows:
                a) doc_len: list of integer. Each element represents the length of a document.
                b) tf: list of dictionaries. Each dictionary corresponds to a document as follows:
                                                                    key: term
                                                                    value: normalized term frequency (by the length of document)                                                                                           
                c) df: dictionary representing the document frequency as follows:
                                                                    key: term
                                                                    value: document frequency
    """    
    # YOUR CODE HERE  
    doc_len = [len(d) for d in data]
    tf = []
    df = {}
    diff = defaultdict(int)
    index = 0
    for d in data:
      for word in set(d):
        diff[word] +=1
      c = Counter(d)
      tmp = {}
      for key in c:
        tmp[key] = c[key]/doc_len[index]
      tf.append(tmp)      
      index+=1
    df = dict(diff)
    return doc_len,tf,df

# tests - preprocess data for naive bm25
doc_len,tf,df = bm25_preprocess(clean_data)
assert df['blue'] == 4
assert sum(tf[0].values()) == 1
assert sum(doc_len) > sum(df.values())

"""**YOUR TASK (5 POINTS):** Complete the implementation of `calc_idf` and `_score` at the BM25 class.

"""

import math

class BM25:
    """
    Best Match 25.

    Parameters to tune
    ----------
    k1 : float, default 1.5

    b : float, default 0.75

    Attributes
    ----------
    tf_ : list[dict[str, int]]
        Term Frequency per document. So [{'hi': 1}] means
        the first document contains the term 'hi' 1 time.
        The frequnecy is normilzied by the max term frequency for each document.

    doc_len_ : list[int]
        Number of terms per document. So [3] means the first
        document contains 3 terms. 
        
    df_ : dict[str, int]
        Document Frequency per term. i.e. Number of documents in the
        corpus that contains the term.       

    avg_doc_len_ : float
        Average number of terms for documents in the corpus.

    idf_ : dict[str, float]
        Inverse Document Frequency per term.
    """

    def __init__(self,doc_len,df,tf=None,k1=1.5, b=0.75):
        self.b = b
        self.k1 = k1
        self.tf_ = tf
        self.doc_len_ = doc_len
        self.df_ = df
        self.N_ = len(doc_len)
        self.avgdl_ = sum(doc_len) / len(doc_len)        
        

    def calc_idf(self,query):
        """
        This function calculate the idf values according to the BM25 idf formula for each term in the query.
        
        Parameters:
        -----------
        query: list of token representing the query. For example: ['look', 'blue', 'sky']
        
        Returns:
        -----------
        idf: dictionary of idf scores. As follows: 
                                                    key: term
                                                    value: bm25 idf score
        """
        # YOUR CODE HERE
        result_sum = 0
        nq = [0]
        for word in query:
          if word in self.df_.keys():
            nq.append(self.df_[word])
        nq = max(nq)
        return math.log( ((self.N_ - nq +0.5) / (nq + 0.5)) + 1)

        

    def search(self, queries):
        """
        This function use the _score function to calculate the bm25 score for all queries provided.
        
        Parameters:
        -----------
        queries: list of lists. Each inner list is a list of tokens. For example:
                                                                                    [
                                                                                        ['look', 'blue', 'sky'],
                                                                                        ['likes', 'blue', 'sun'],
                                                                                        ['likes', 'diamonds']
                                                                                    ]

        Returns:
        -----------
        list of scores of bm25
        """
        
        scores = []
        for query in queries:    
            self.idf_ = self.calc_idf(query)      
            scores.append([self._score(query, doc_id) for doc_id in range(self.N_)])
        return scores

class BM25(BM25):

    def _score(self, query, doc_id):
        """
        This function calculate the bm25 score for given query and document.
        
        Parameters:
        -----------
        query: list of token representing the query. For example: ['look', 'blue', 'sky']
        doc_id: integer, document id.
        
        Returns:
        -----------
        score: float, bm25 score.
        """
        # YOUR CODE HERE
        res = 0
        f_qi_D=0
        for term in query:
          if term in self.tf_[doc_id]:
            f_qi_D += 1
          else:
            f_qi_D = 0
          exp = 1 - self.b + self.b * (self.doc_len_[doc_id]/ self.avgdl_)
          res += (self.idf_ * ( ( f_qi_D * (self.k1 + 1)) / ( f_qi_D + self.k1 * exp)))
        return res

"""Let's create a new instance of BM25, and calculte its score for all queries. 
Pay attetnion - we need to tokenize and filter the stopwords from the original queries.
"""

bm25 = BM25(tf=tf,doc_len=doc_len,df=df)

# Remove tokenizing and remove stopwords from queries
clean_queries = [tokenize(query) for query in queries]
BM25_res = bm25.search(clean_queries)
BM25_df = pd.DataFrame(data = BM25_res,index = [query_id for query_id in range(len(clean_queries))] ,columns = [doc_id for doc_id in range(len(data))])

# tests BM25
assert BM25_df[0][0] == BM25_df[0][2]
assert BM25_df[2][0] == BM25_df[2][2]
assert BM25_df[3][0] != BM25_df[3][2]
assert (BM25_df>1).any().any() == True
assert (BM25_df==0).any().any() == True
assert BM25_df.sum().argmax() == 5
assert BM25_df.sum().argmin() == 1
assert sum(bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[0]) == 0
assert (bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[1][-1] > BM25_df[5][2])
assert (bm25.search([['amit','livne'],['lucy','likes','blue','bright','diamonds']])[1][2] > BM25_df[1][2])

"""We can now search for top-N documents for each query

**YOUR TASK (5 POINTS):**  Complete the implementation of `top_N_documents`, which sort and filter the top N docuemnts (by score) for each query.
"""

def top_N_documents(df,N):
    """
    This function sort and filter the top N docuemnts (by score) for each query.
    
    Parameters
    ----------    
    df: DataFrame (queries as rows, documents as columns)
    N: Integer (how many document to retrieve for each query)    

    Returns:
    ----------
    top_N: dictionary is the following stracture:
          key - query id.
          value - sorted (according to score) list of pairs lengh of N. Each pair within the list provide the following information (doc id, score)
    """    
    # YOUR CODE HERE
    top_N_dict = {}
    df = pd.DataFrame(df, index=[i for i in range(df.shape[0])],
                      columns=[i for i in range(df.shape[1])])
    for index, row in df.iterrows():
      sorted_ = row.sort_values(ascending=False)[0:N]
      indexs = sorted_.index.to_list()
      vals = sorted_.values.tolist()
      top_N_dict[index] = list(zip(indexs, vals))
    return top_N_dict

# test - topN documents
top_2_docs = top_N_documents(cosine_sim_df,2)
top_10_docs = top_N_documents(cosine_sim_df,10)
assert len(top_2_docs[0]) == 2
assert len(top_2_docs[0][0]) == 2
assert (top_2_docs[0][0][1] > top_N_documents(cosine_sim_df,2)[0][1][1])
assert (top_10_docs[0][-1][-1] == 0)
assert (top_10_docs[0][-1][-1] != top_10_docs[1][-1][-1])
assert (top_10_docs[0][0][-1] != top_10_docs[1][0][-1])

"""The top-N functionality runs on all document and create a large DataFrame. What will happen if the number of documents is much larger? 
Will this function be efficient? 

Next, we will build an inverted index (like we did in assignment two). Then, we will use the inverted index utilizing the posting list to create a candidate list of documents per query. 
Therefore, we will narrow the documents relevant per query.

Only then will we calculate the TF-IDF and cosine similarity.

## Tasks 2: Implement compound ranking based on title and body using the inverted index (20 points).

In order to do so, we will first perform parsing followed by building an Inverted index.
Then we will create a searching and ranking mechanism.

In this task, you will use Cranfield Corpus. But we need to do some setups in advance. 

**Setups:**

*Make sure you upload the carnfield.py file. attached with this assignment*

First, we will load it into the following data structures:

* cran_txt_data - list of tuples. An example is provided below. Hold information regards the carnfield dataset.
* cran_qry_rel_data - list of tuples. An example is provided below. Holds information regards the queries.

We have already done all the load and preprocessing of the data for you.

We split the queries data into the training and test sets.
Next, we build two inverted indices (one built upon title information and the second on the text\body of the documents). 

**After finishing the setups, we implement the ranking mechanism (TF-IDF and BM25 build upon inverted index).**

### Cranfield Corpus

You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).  <br>

**Parsing**

For detailed information about the parsing of this corpus look at [ this Notebook](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/00_Data/CranfieldCorpus.ipynb) or, for parsing in general, read [this guide](https://pragmalingu.de/docs/guides/how-to-parse). An overview of the format of the files can be found here: [Data Sets](https://pragmalingu.de/docs/guides/data-comparison)
"""

import carnfield

cran_txt_data,cran_qry_rel_data = carnfield.main()

"""### Data seperation
In real-world scenarios, we do not know all the queries in advance. Thus, we are familiar with some queries and can use them to tune the parameters of our model (e.g., the k and b for BM25 or term that appears at least n times within the data, etc.'). We will split the queries into two sets to simulate this scenario: Train and Test.

You can and should use the data from the train set to tune the parameters of your retrieval model. **You should not use the test data for tuning.**


The test data should be used to evaluate your model according to the metrics in the following section (e.g., precision, recall etc.')

Example of cran_txt_data:
"""

# list(islice(cran_txt_data.items(), 1))

"""Example of cran_qry_rel_data. 
For each query id you can see the question and the relevance assessments is a list of tuples of relevant document. The tuple structure is as follows: (document id, relevance score). Therefore a tuple such as (184,2) indicates that the document id 184 relevance score is 2.
"""

# list(islice(cran_qry_rel_data.items(), 1))

"""Split to train and test queries"""

cran_qry_rel_data_train = list(islice(cran_qry_rel_data.items(), 0,180))
cran_qry_rel_data_test = list(islice(cran_qry_rel_data.items(), 180,225))

"""Next, we will use the inverted index from assignment 2 (with some minor adaptations).
We save a dictionary named `DL`, which fetches the document length of each document.

Upon using the index functionality of add_doc, `DL` needs to be updated as well.

### Inverted index (code from assignment 2)

#### Helper classes (code from assignment 2)
"""

# Let's start with a small block size of 30 bytes just to test things out. 
BLOCK_SIZE = 199998

class MultiFileWriter:
    """ Sequential binary writer to multiple files of up to BLOCK_SIZE each. """
    def __init__(self, base_dir, name):
        self._base_dir = Path(base_dir)
        self._name = name
        self._file_gen = (open(self._base_dir / f'{name}_{i:03}.bin', 'wb') 
                          for i in count())
        self._f = next(self._file_gen)
    
    def write(self, b):
      locs = []
      while len(b) > 0:
        pos = self._f.tell()
        remaining = BLOCK_SIZE - pos
        # if the current file is full, close and open a new one.
        if remaining == 0:  
          self._f.close()
          self._f = next(self._file_gen)
          pos, remaining = 0, BLOCK_SIZE
        self._f.write(b[:remaining])
        locs.append((self._f.name, pos))
        b = b[remaining:]
      return locs

    def close(self):
      self._f.close()

class MultiFileReader:
  """ Sequential binary reader of multiple files of up to BLOCK_SIZE each. """
  def __init__(self):
    self._open_files = {}

  def read(self, locs, n_bytes):
    b = []
    for f_name, offset in locs:
      if f_name not in self._open_files:
        self._open_files[f_name] = open(f_name, 'rb')
      f = self._open_files[f_name]
      f.seek(offset)
      n_read = min(n_bytes, BLOCK_SIZE - offset)
      b.append(f.read(n_read))
      n_bytes -= n_read
    return b''.join(b)
  
  def close(self):
    for f in self._open_files.values():
      f.close()

  def __exit__(self, exc_type, exc_value, traceback):
    self.close()
    return False

TUPLE_SIZE = 6       # We're going to pack the doc_id and tf values in this 
                     # many bytes.
TF_MASK = 2 ** 16 - 1 # Masking the 16 low bits of an integer

DL = {}  # We're going to update and calculate this after each document. This will be usefull for the calculation of AVGDL (utilized in BM25) 
class InvertedIndex:  
  def __init__(self, docs={}):
    """ Initializes the inverted index and add documents to it (if provided).
    Parameters:
    -----------
      docs: dict mapping doc_id to list of tokens
    """
    # stores document frequency per term
    self.df = Counter()
    # stores total frequency per term
    self.term_total = Counter()
    # stores posting list per term while building the index (internally), 
    # otherwise too big to store in memory.
    self._posting_list = defaultdict(list)
    # mapping a term to posting file locations, which is a list of 
    # (file_name, offset) pairs. Since posting lists are big we are going to
    # write them to disk and just save their location in this list. We are 
    # using the MultiFileWriter helper class to write fixed-size files and store
    # for each term/posting list its list of locations. The offset represents 
    # the number of bytes from the beginning of the file where the posting list
    # starts. 
    self.posting_locs = defaultdict(list)
    
    for doc_id, tokens in docs.items():
      self.add_doc(doc_id, tokens)

  def add_doc(self, doc_id, tokens):
    """ Adds a document to the index with a given `doc_id` and tokens. It counts
        the tf of tokens, then update the index (in memory, no storage 
        side-effects).
    """
    DL[(doc_id)] = DL.get(doc_id,0) + (len(tokens))
    w2cnt = Counter(tokens)
    self.term_total.update(w2cnt)
    max_value = max(w2cnt.items(), key=operator.itemgetter(1))[1]    
    # frequencies = {key: value/max_value for key, value in frequencies.items()}
    for w, cnt in w2cnt.items():        
        self.df[w] = self.df.get(w, 0) + 1                
        self._posting_list[w].append((doc_id, cnt))


  def write(self, base_dir, name):
    """ Write the in-memory index to disk and populate the `posting_locs`
        variables with information about file location and offset of posting
        lists. Results in at least two files: 
        (1) posting files `name`XXX.bin containing the posting lists.
        (2) `name`.pkl containing the global term stats (e.g. df).
    """
    #### POSTINGS ####
    self.posting_locs = defaultdict(list)
    with closing(MultiFileWriter(base_dir, name)) as writer:
      # iterate over posting lists in lexicographic order
      for w in sorted(self._posting_list.keys()):
        self._write_a_posting_list(w, writer, sort=True)
    #### GLOBAL DICTIONARIES ####
    self._write_globals(base_dir, name)

  def _write_globals(self, base_dir, name):
    with open(Path(base_dir) / f'{name}.pkl', 'wb') as f:
      pickle.dump(self, f)

  def _write_a_posting_list(self, w, writer, sort=False):
    # sort the posting list by doc_id
    pl = self._posting_list[w]
    if sort:
      pl = sorted(pl, key=itemgetter(0))
    # convert to bytes    
    b = b''.join([(int(doc_id) << 16 | (tf & TF_MASK)).to_bytes(TUPLE_SIZE, 'big')
                  for doc_id, tf in pl])
    # write to file(s)
    locs = writer.write(b)
    # save file locations to index
    self.posting_locs[w].extend(locs) 

  def __getstate__(self):
    """ Modify how the object is pickled by removing the internal posting lists
        from the object's state dictionary. 
    """
    state = self.__dict__.copy()
    del state['_posting_list']
    return state

  @staticmethod
  def read_index(base_dir, name):
    with open(Path(base_dir) / f'{name}.pkl', 'rb') as f:
      return pickle.load(f)

  @staticmethod
  def delete_index(base_dir, name):
    path_globals = Path(base_dir) / f'{name}.pkl'
    path_globals.unlink()
    for p in Path(base_dir).rglob(f'{name}_*.bin'):
      p.unlink()

"""In this assignment we will generate two indexes. 
One for titles and one for body (e.g., text).
First, we need to tokenize the title and body textual information (e.g., text). 
Note: it can take several minutes to tokenize it.

Next, we will create two seperated indexes.

"""

cran_txt_data_titles = {k: tokenize(v['title']) for k,v in cran_txt_data.items()}
cran_txt_data_text = {k: tokenize(v['text']) for k,v in cran_txt_data.items()}

"""Next, we need to preprocess the text of the queries for both training and test sets."""

cran_txt_query_text_train = {q[0]: tokenize(q[1]['question']) for q in cran_qry_rel_data_train}
cran_txt_query_text_test = {q[0]: tokenize(q[1]['question']) for q in cran_qry_rel_data_test}

"""#### Creating and writing the index"""

index_titles = InvertedIndex(docs=cran_txt_data_titles)
index_text = InvertedIndex(docs=cran_txt_data_text)

# create directories for the different indices 
# !mkdir body_index title_index

index_titles.write('title_index','title')
index_text.write('body_index','body')

"""### Reading data from posting (code taken from assignment 2)"""

class InvertedIndex(InvertedIndex):
  
  def posting_lists_iter(self):
    """ A generator that reads one posting list from disk and yields 
        a (word:str, [(doc_id:int, tf:int), ...]) tuple.
    """
    with closing(MultiFileReader()) as reader:
      for w, locs in self.posting_locs.items():
        # read a certain number of bytes into variable b
        b = reader.read(locs, self.df[w] * TUPLE_SIZE)
        posting_list = []
        # convert the bytes read into `b` to a proper posting list.
        
        for i in range(self.df[w]):
          doc_id = int.from_bytes(b[i*TUPLE_SIZE:i*TUPLE_SIZE+4], 'big')
          tf = int.from_bytes(b[i*TUPLE_SIZE+4:(i+1)*TUPLE_SIZE], 'big')
          posting_list.append((doc_id, tf))
        
        yield w, posting_list

def get_posting_gen(index):
    """
    This function returning the generator working with posting list.
    
    Parameters:
    ----------
    index: inverted index    
    """
    words, pls = zip(*index.posting_lists_iter())
    return words,pls

"""**Example of use**

First, we will load the title index. 

Next, we will search for a given term in both lists (words, and pls)

Remineder: pls is the information from the posting list.
Each value within the `words` list has a corresponding value in the `pls` list.
The value within a `pls` list is a list of tuples. Each tuple in the following format (x,y). Where x represent the document id, and `y` represent the occurence of the term in the document. 

In this example we are seeking the first term in the `words` list, and we can observe the is appears in 4 different documents. 
Moreover, we can observe that in document 807 it appears twice.
"""

idx_title = InvertedIndex.read_index('title_index', 'title')
idx_body = InvertedIndex.read_index('body_index', 'body')
# read posting lists from disk
words, pls = zip(*idx_title.posting_lists_iter())

"""Let's check for example the document number `807`. 
We needed its document length (DL) which is suppose to be the length of its title + the length of the text of the document. 
Let's verify it.
"""

#Let's check the title of document 807
cran_txt_data_titles['807']

"""Clearly `000` should not appear twice.
Let have a second look on the original document and understand weather it's a mistake within a tokenization step or in the original title.
"""

cran_txt_data['807']['title']

"""As we could have guessed, the token `000` does not appear twice in this document.

**Reminder: garbage in = garbage out.**

**For this assignment, it is ok to leave it as is. However, verify this part when you are working on your project.**
"""

assert DL['807'] == len(cran_txt_data_titles['807']) + len(cran_txt_data_text['807'])
assert len(DL) == len(cran_txt_data_titles)

"""### Ranking
In this section, you will be given a query or queries and return a ranked list of documents to retrieve.
In this assignment, we are experimenting with two methods. TF-IDF and BM25. 

**We will use the inverted index in order to do so and will not utilize the whole corpus in advance.**

#### TF-IDF for carnfield data (10 points)

Bellow cells contain the following functions: 

*   *generate_query_tfidf_vector* - Generate a vector representing the query
*   *get_candidate_documents_and_scores* - Generate a dictionary representing a pool of candidate documents for a given query.
*   *generate_document_tfidf_matrix* - Generate a DataFrame of tfidf scores for a given query.
*   *cosine_similarity* - Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q). **You will impelement this function (5 points)**

*   *get_top_n* - Sort and return the highest N documents according to the cosine similarity score.

*   *get_topN_score_for_queries* - Generate a dictionary that gather for every query its topN score. **You will impelement this function (5 points)**
"""

def generate_query_tfidf_vector(query_to_search,index):
    """ 
    Generate a vector representing the query. Each entry within this vector represents a tfidf score.
    The terms representing the query will be the unique terms in the index.

    We will use tfidf on the query as well. 
    For calculation of IDF, use log with base 10.
    tf will be normalized based on the length of the query.    

    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). 
                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.
    
    Returns:
    -----------
    vectorized query with tfidf scores
    """
    
    epsilon = .0000001
    total_vocab_size = len(index.term_total)
    Q = np.zeros((total_vocab_size))
    term_vector = list(index.term_total.keys())    
    counter = Counter(query_to_search)
    for token in np.unique(query_to_search):
        if token in index.term_total.keys(): #avoid terms that do not appear in the index.               
            tf = counter[token]/len(query_to_search) # term frequency divded by the length of the query
            df = index.df[token]            
            idf = math.log((len(DL))/(df+epsilon),10) #smoothing
            
            try:
                ind = term_vector.index(token)
                Q[ind] = tf*idf                    
            except:
                pass
    return Q

def get_candidate_documents_and_scores(query_to_search,index,words,pls):
    """
    Generate a dictionary representing a pool of candidate documents for a given query. This function will go through every token in query_to_search
    and fetch the corresponding information (e.g., term frequency, document frequency, etc.') needed to calculate TF-IDF from the posting list.
    Then it will populate the dictionary 'candidates.'
    For calculation of IDF, use log with base 10.
    tf will be normalized based on the length of the document.
    
    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). 
                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.

    words,pls: generator for working with posting.
    Returns:
    -----------
    dictionary of candidates. In the following format:
                                                               key: pair (doc_id,term)
                                                               value: tfidf score. 
    """
    candidates = {}
    N = len(DL)        
    for term in np.unique(query_to_search):        
        if term in words:            
            list_of_doc = pls[words.index(term)]                        
            normlized_tfidf = [(doc_id,(freq/DL[str(doc_id)])*math.log(N/index.df[term],10)) for doc_id, freq in list_of_doc]           
                        
            for doc_id, tfidf in normlized_tfidf:
                candidates[(doc_id,term)] = candidates.get((doc_id,term), 0) + tfidf               
        
    return candidates

def generate_document_tfidf_matrix(query_to_search,index,words,pls):
    """
    Generate a DataFrame `D` of tfidf scores for a given query. 
    Rows will be the documents candidates for a given query
    Columns will be the unique terms in the index.
    The value for a given document and term will be its tfidf score.
    
    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance (e.g., lower case, filtering stopwords, etc.'). 
                     Example: 'Hello, I love information retrival' --->  ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.

    words,pls: generator for working with posting.
    Returns:
    -----------
    DataFrame of tfidf scores.
    """
    
    total_vocab_size = len(index.term_total)
    candidates_scores = get_candidate_documents_and_scores(query_to_search,index,words,pls) #We do not need to utilize all document. Only the docuemnts which have corrspoinding terms with the query.
    unique_candidates = np.unique([doc_id for doc_id, freq in candidates_scores.keys()])
    D = np.zeros((len(unique_candidates), total_vocab_size))
    D = pd.DataFrame(D)
    
    D.index = unique_candidates
    D.columns = index.term_total.keys()

    for key in candidates_scores:
        tfidf = candidates_scores[key]
        doc_id, term = key    
        D.loc[doc_id][term] = tfidf

    return D

"""**YOUR TASK (5 POINTS):** Complete the implementation of `cosine_similarity`. This function calculate the cosine similarity for each candidate document in D and a given query (e.g., Q) and return a dictionary of cosine similary scores.

**Note:** for this task you cannot use sklearn. However, you can use pandas or numpy.
"""

def cosine_similarity(D,Q):
    """
    Calculate the cosine similarity for each candidate document in D and a given query (e.g., Q).
    Generate a dictionary of cosine similarity scores 
    key: doc_id
    value: cosine similarity score
    
    Parameters:
    -----------
    D: DataFrame of tfidf scores.

    Q: vectorized query with tfidf scores
    
    Returns:
    -----------
    dictionary of cosine similarity score as follows:
                                                                key: document id (e.g., doc_id)
                                                                value: cosine similarty score.
    """
    # YOUR CODE HERE
    Q = pd.DataFrame(Q)
    Q = Q.rename(columns={0: "rate"})
    selected_rows = Q[Q['rate']> 0.0]
    ans_dict = {}
    wiq = selected_rows.loc[:, 'rate'].transform(lambda x: math.pow(1,2)).sum()
    D = D.iloc[:,list(selected_rows.index)]
    for doc_id, row in D.iterrows():
      inner_product = 0
      wij = row.transform(lambda x: math.pow(x,2)).sum()
      inner_product = row.sum()
      denumerator = (math.sqrt(wiq*wij))
      terms_rate = inner_product / denumerator
      ans_dict[doc_id] = round(float(terms_rate),1)
    return ans_dict

def get_top_n(sim_dict,N=3):
    """ 
    Sort and return the highest N documents according to the cosine similarity score.
    Generate a dictionary of cosine similarity scores 
   
    Parameters:
    -----------
    sim_dict: a dictionary of similarity score as follows:
                                                                key: document id (e.g., doc_id)
                                                                value: similarity score. We keep up to 5 digits after the decimal point. (e.g., round(score,5))

    N: Integer (how many documents to retrieve). By default N = 3
    
    Returns:
    -----------
    a ranked list of pairs (doc_id, score) in the length of N.
    """
    
    return sorted([(doc_id,round(score,5)) for doc_id, score in sim_dict.items()], key = lambda x: x[1],reverse=True)[:N]

"""**YOUR TASK (5 POINTS)**: Complete the implementation of `get_topN_score_for_queries`. This function generate a dictionary that gather for every query its topN score, based on cosine similarity."""

def get_topN_score_for_queries(queries_to_search,index,N=3):
    """ 
    Generate a dictionary that gathers for every query its topN score.
    
    Parameters:
    -----------
    queries_to_search: a dictionary of queries as follows: 
                                                        key: query_id
                                                        value: list of tokens.
    index:           inverted index loaded from the corresponding files.    
    N: Integer. How many documents to retrieve. This argument is passed to the topN function. By default N = 3, for the topN function. 
    
    Returns:
    -----------
    return: a dictionary of queries and topN pairs as follows:
                                                        key: query_id
                                                        value: list of pairs in the following format:(doc_id, score). 
    """
    # YOUR CODE HERE
    top_n_dict = {}
    for query_index, query_to_search in queries_to_search.items():
      words, pls = get_posting_gen(index)
      document_tfidf_matrix = generate_document_tfidf_matrix(query_to_search,index,words,pls)
      query_tfidf_vec = generate_query_tfidf_vector(query_to_search,index)
      sim_dict = cosine_similarity(D=document_tfidf_matrix, Q=query_tfidf_vec)
      # print(sim_dict)
      top_n_dict[query_index] = get_top_n(sim_dict=sim_dict, N=N)
    return top_n_dict

tfidf_queries_score_train = get_topN_score_for_queries(cran_txt_query_text_train,idx_title)

#tests 
assert len(tfidf_queries_score_train)==180
assert 0 not in tfidf_queries_score_train.keys()
assert len(tfidf_queries_score_train[5])==3
assert tfidf_queries_score_train[172][0][1] == tfidf_queries_score_train[172][1][1]
assert tfidf_queries_score_train[14][0][1] == tfidf_queries_score_train[172][1][1]

"""For query 172 we can observe two document with cosine similarity score of 1. Let's have a glance on this query and documents for making sure it makes sense"""

print('relevnt documents and tfidf score for query number 172 :',tfidf_queries_score_train[172])
print('query: ' ,cran_txt_query_text_train[172])
print('docuemnt 320: ', cran_txt_data_titles['320'])
print('docuemnt 320: ', cran_txt_data_titles['321'])
print('docuemnt 322: ', cran_txt_data_titles['322'])

"""#### BM25 for carnfield data (10 points)
As a reminder:

To use BM25 we will need to following parameters:

* $k1$ and $b$ - free parameters
* $f(t_i,D)$ - term frequency of term $t_i$ in document $D$
* |$D$|- is the length of the document $D$ in terms 
* $avgdl$ -  average document length
* $IDF$ - which is calculted as follows: $ln(\frac{(N-n(t_i)+0.5}{n(t_i)+0.5)}+1)$, where $N$ is the total number of documents in the collection, and $n(t_i)$ is the number of documents containing $t_i$ (e.g., document frequency (df)).

Now, we will use the inverted index to fetch this information.

**We need to check only documents which are 'candidates' for a given query.**

We can create a simpler version of get candidate functions. (We do not need to calculate the TFIDF scores)
"""

def get_candidate_documents(query_to_search,index,words,pls):
    """
    Generate a dictionary representing a pool of candidate documents for a given query. 
    
    Parameters:
    -----------
    query_to_search: list of tokens (str). This list will be preprocessed in advance
                     (e.g., lower case, filtering stopwords, etc.'). 
                     Example: 'Hello, I love information retrival' --->
                       ['hello','love','information','retrieval']

    index:           inverted index loaded from the corresponding files.

    words,pls: generator for working with posting.
    Returns:
    -----------
    list of candidates. In the following format:
                                                key: pair (doc_id,term)
                                                value: tfidf score. 
    """
    candidates = []    
    for term in np.unique(query_to_search):
        if term in words:        
            current_list = (pls[words.index(term)])                
            candidates += current_list    
    return np.unique(candidates)

"""**YOUR TASK (10 POINTS):** Complete the implementation of `search` at the BM25 class.
Differently, from previous implememntation, this time you should check only documents which are 'candidates' for a given query. 

"""

import math
from itertools import chain
import time
# When preprocessing the data have a dictionary of document length for each document saved in a variable called `DL`.
class BM25_from_index:
    """
    Best Match 25.    
    ----------
    k1 : float, default 1.5

    b : float, default 0.75

    index: inverted index
    """

    def __init__(self,index,k1=1.5, b=0.75):
        self.b = b
        self.k1 = k1
        self.index = index
        self.N = len(DL)
        self.AVGDL = sum(DL.values())/self.N
        self.words, self.pls = zip(*self.index.posting_lists_iter())        

    def calc_idf(self,list_of_tokens):
        """
        This function calculate the idf values according to the BM25 idf formula for each term in the query.
        
        Parameters:
        -----------
        query: list of token representing the query. For example: ['look', 'blue', 'sky']
        
        Returns:
        -----------
        idf: dictionary of idf scores. As follows: 
                                                    key: term
                                                    value: bm25 idf score
        """        
        idf = {}        
        for term in list_of_tokens:            
            if term in self.index.df.keys():
                n_ti = self.index.df[term]
                idf[term] = math.log(1 + (self.N - n_ti + 0.5) / (n_ti + 0.5))
            else:
                pass                             
        return idf
        

    def search(self, queries,N=3):
        """
        This function calculate the bm25 score for given query and document.
        We need to check only documents which are 'candidates' for a given query. 
        This function return a dictionary of scores as the following:
                                  key: query_id
                                  value: a ranked list of pairs (doc_id, score) in the length of N.
        
        Parameters:
        -----------
        query: list of token representing the query. For example: ['look', 'blue', 'sky']
        doc_id: integer, document id.
        
        Returns:
        -----------
        score: float, bm25 score.
        """
        # YOUR CODE HERE
        scores = {}        
        self.idf = self.calc_idf(list(set(chain.from_iterable(queries.values()))))
        # key = query_id, val = list of tuple(candidate_doc_id, bm25value)
        # for 
        for query_index, query in queries.items():
            candidate_documents = get_candidate_documents(query, self.index,self.words, self.pls)
            # self.idf = self.calc_idf(query)      
            # scores[query_index] = get_top_n(dict([(doc_id, self._score(query, doc_id)) for doc_id in candidate_documents]),N)
            scores_lst = [(doc_id,self._score(query, doc_id)) for doc_id in candidate_documents]
            scores[query_index] = sorted(scores_lst, key=lambda x: x[1], reverse=True)[:N]
        return scores

    def _score(self, query, doc_id):
        """
        This function calculate the bm25 score for given query and document.
        
        Parameters:
        -----------
        query: list of token representing the query. For example: ['look', 'blue', 'sky']
        doc_id: integer, document id.
        
        Returns:
        -----------
        score: float, bm25 score.
        """        
        score = 0.0        
        doc_len = DL[str(doc_id)]        
             
        for term in query:
            if term in self.index.term_total.keys():                
                term_frequencies = dict(self.pls[self.words.index(term)])                
                if doc_id in term_frequencies.keys():            
                    freq = term_frequencies[doc_id]
                    numerator = self.idf[term] * freq * (self.k1 + 1)
                    denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.AVGDL)
                    score += (numerator / denominator)
        return score

bm25_title = BM25_from_index(idx_title)
bm25_queries_score_train = bm25_title.search(cran_txt_query_text_train)

#tests
assert len(bm25_queries_score_train)==180
assert 0 not in bm25_queries_score_train.keys()
assert len(bm25_queries_score_train[5])==3
assert bm25_queries_score_train[172][0][1] != bm25_queries_score_train[172][1][1]
assert bm25_queries_score_train[14][0][1] != bm25_queries_score_train[172][1][1]

"""## Task 3: Using weights of title and body scores (15 points)

Reminder: we are building on the training set.
Later you need to optimize the parameters and run the best parameters on the test set.

Now we will experiment with two sets of results. 
The first corresponds to results from the title index. 
The second corresponds to the results from the body index.

We need to merge them into a single result set.

**YOUR TASK (10 POINTS):** Complete the implementation of `merge_results`.
This function merge and sort documents retrieved by its weighte score (e.g., title and body).
"""

def merge_results(title_scores,body_scores,title_weight=0.5,text_weight=0.5,N = 3):    
    """
    This function merge and sort documents retrieved by its weighte score (e.g., title and body). 

    Parameters:
    -----------
    title_scores: a dictionary build upon the title index of queries and tuples representing scores as follows: 
                                                                            key: query_id
                                                                            value: list of pairs in the following format:(doc_id,score)
                
    body_scores: a dictionary build upon the body/text index of queries and tuples representing scores as follows: 
                                                                            key: query_id
                                                                            value: list of pairs in the following format:(doc_id,score)
    title_weight: float, for weigted average utilizing title and body scores
    text_weight: float, for weigted average utilizing title and body scores
    N: Integer. How many document to retrieve. This argument is passed to topN function. By default N = 3, for the topN function. 
    
    Returns:
    -----------
    dictionary of querires and topN pairs as follows:
                                                        key: query_id
                                                        value: list of pairs in the following format:(doc_id,score). 
    """
    # YOUR CODE HERE
    merge_results_dict = {}
    for i in title_scores.keys():
      dict_scores = dict([(k,v*title_weight) for k,v in title_scores[i]])
      for k,v in body_scores[i]:
        if k in dict_scores.keys():
          dict_scores[k] += v*text_weight
        else:
          dict_scores[k] = v*text_weight
      lst = list(dict_scores.items())
      merge_results_dict[i] = [(k,v) for k, v in sorted(lst, key=lambda item: item[1], reverse=True)][:N]


    # merge_results_dict[i] = get_top_n(dict_title_scores, N)
    # lst_body_scores = [(k,v*title_weight) for k,v in body_scores[i]]

    # merge_results_dict = {}
    # for i in range(1,len(title_scores)+1):
    #   set_title_scores = set([(val[0], val[1]*title_weight) for val in title_scores[i]])
    #   set_body_scores = set([(val[0], val[1]*text_weight) for val in body_scores[i]])
    #   intersec = set_title_scores.intersection(set_body_scores)
    #   intersec = set([(val[0], val[1]) for val in intersec])
    #   diff = list(set_title_scores.difference(set_body_scores))
    #   intersec = list(intersec)
    #   merge_results_dict[i]  = diff + intersec
    #   merge_results_dict[i] = [(k,v) for k, v in sorted(merge_results_dict[i], key=lambda item: item[1], reverse=True)][:N]
    # merge_results_dict = {k: v for k, v in sorted(merge_results_dict.items(), key=lambda item: item[0], reverse=True)} 

    # merge_results_dict = {}
    # for i in range(1,len(title_scores)+1):
    #   set_title_scores = set([(val[0], val[1]) for val in title_scores[i]])
    #   set_body_scores = set([(val[0], val[1]) for val in body_scores[i]])
    #   intersec = set_title_scores.intersection(set_body_scores)
    #   intersec = set([(val[0], val[1]) for val in intersec])
    #   diff = list(set_title_scores.difference(set_body_scores))
    #   intersec = list(intersec)
    #   merge_results_dict[i]  = dict(diff + intersec)
    #   merge_results_dict[i] = get_top_n(sim_dict=merge_results_dict[i], N=N)
    #   # merge_results_dict[i] = get_top_N([(k,v) for k, v in sorted(merge_results_dict[i], key=lambda item: item[1], reverse=True)])
    # merge_results_dict = {k: v for k, v in sorted(merge_results_dict.items(), key=lambda item: item[0], reverse=True)} 



    # merge_results_dict = {}
    # for i in title_scores.keys():
    #   set_title_scores = set([(val[0], val[1]) for val in title_scores[i]])
    #   set_body_scores = set([(val[0], val[1]) for val in body_scores[i]])
    #   intersec = set_title_scores.intersection(set_body_scores)
    #   intersec = list(set([(val[0], val[1]*text_weight*title_weight) for val in intersec]))
    #   diff = set_title_scores.difference(set_body_scores)
    #   diff_title = [(val[0], val[1]*title_weight) for val in list(set_title_scores.intersection(diff))]
    #   diff_body  = [(val[0], val[1]*text_weight) for val in list(set_body_scores.intersection(diff))]
    #   a  = dict(set(diff_title + intersec + diff_body))
    #   b = get_top_n(sim_dict=a, N=N)
    #   merge_results_dict[i] = [(k,v) for k, v in sorted(b, key=lambda item: item[0], reverse=True)]
    # merge_results_dict = {k: v for k, v in sorted(merge_results_dict.items(), key=lambda item: item[0], reverse=True)} 
    # merge_results_dict = dict(get_top_n(merge_results_dict, N))

    
    
    # print({k: v for k, v in sorted(merge_results_dict.items(), key=lambda item: max(item[1], key=lambda k: k[1]),  reverse=True)}.items())
    # return {k: v for k, v in sorted(merge_results_dict.items(), key=lambda item: max(item[1], key=lambda k: k[0]), reverse=True)} 
    return merge_results_dict

bm25_title = BM25_from_index(idx_title)
bm25_body = BM25_from_index(idx_body)

bm25_queries_score_train_title = bm25_title.search(cran_txt_query_text_train)
bm25_queries_score_train_body = bm25_body.search(cran_txt_query_text_train)

#tests
w1,w2 = 0.5, 0.5
w3,w4 = 0.25,0.75

half_and_half = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,w1,w2)       
assert len(half_and_half[2]) == 3
assert type(half_and_half) == dict
assert type(half_and_half[2]) == list
assert len(half_and_half) == 180
assert half_and_half[2][0][1] == 0.5 * (bm25_queries_score_train_title[2][-1][1]+ bm25_queries_score_train_body[2][0][1])

quarter_and_three_quarters = merge_results(bm25_queries_score_train_title,bm25_queries_score_train_body,0.25,0.75)        
assert quarter_and_three_quarters[2][0][1] == (w3 * bm25_queries_score_train_title[2][-1][1] + w4 * bm25_queries_score_train_body[2][0][1])
assert {k for k,v in half_and_half[16]} != {k for k,v in quarter_and_three_quarters[16]}
assert len({k for k,v in half_and_half[16]}.union({k for k,v in quarter_and_three_quarters[16]})) < (len({k for k,v in half_and_half[16]}) + len({k for k,v in quarter_and_three_quarters[16]}))

"""**YOUR TASK (5 POINTS):** provide three examples of mistakes that the model is making and explanations for why, and describe how you will change the model based on these observations."""

# YOUR EXAMPLES HERE #
"1. 1.24792 != 1.247915"
"2. 2.49583 != 2.2.495827701833633 "
assert True

"""## Task 4: Implement several evaluation metrics (25 points).

At this task, you will need to write the multiple metrics **(without using pre-defined packages)** and evaluate the results on the test set.


Reminder: 
* `carn_queries_train` holds the queries for the training set
* `cran_qry_rel_data_train` holds the relevant data for each query in the training set.
* `carn_queries_test` holds the queries for the test set
* `cran_qry_rel_data_test` holds the relevant data for each query in the test set.
"""

def intersection(l1,l2):      
    """
    This function perform an intersection between two lists.

    Parameters
    ----------
    l1: list of documents. Each element is a doc_id.
    l2: list of documents. Each element is a doc_id.

    Returns:
    ----------
    list with the intersection (without duplicates) of l1 and l2
    """
    return list(set(l1)&set(l2))

"""**YOUR TASK (25 POINTS):** Complete the implementation of the below functions:

"""

def recall_at_k(true_list,predicted_list,k=40):
    """
    This function calculate the recall@k metric.

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------
    float, recall@k with 3 digits after the decimal point.
    """      
    # YOUR CODE HERE
    intersec = intersection(true_list,predicted_list[:k])
    if intersec == []:
      return 0.0
    return round(len(intersec)/ len(true_list),3)

def precision_at_k(true_list,predicted_list,k=40):    
    """
    This function calculate the precision@k metric.

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------
    float, precision@k with 3 digits after the decimal point.
    """      
    # YOUR CODE HERE
    intersec = intersection(true_list,predicted_list[:k])
    if intersec == []:
      return 0.0
    return round(len(intersec)/ k,3)

def r_precision(true_list,predicted_list):
    """
    This function calculate the r-precision metric. No `k` parameter is used.

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score    
    
    Returns:
    -----------
    float, r-precision with 3 digits after the decimal point.
    """    
    # YOUR CODE HERE
    R = len(true_list)
    intersec = intersection(true_list,predicted_list[:R])
    if intersec == []:
      return 0.0
    return round(len(intersec)/R,3)

def reciprocal_rank_at_k(true_list,predicted_list,k=40):
    """
    This function calculate the reciprocal_rank@k metric.        
    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------    
    float, reciprocal rank@k with 3 digits after the decimal point.
    """   
    # YOUR CODE HERE
    intersec = intersection(true_list,predicted_list[:k])
    for pred in predicted_list[:k]:
      if pred in true_list:
        return round(1/(predicted_list.index(pred)+1),3)
    # if intersec == []:
    return 0.0
    # return round(1/(predicted_list.index(intersec[0])+1),3)

def fallout_rate(true_list,predicted_list,k=40):    
    """
    This function calculate the fallout_rate@k metric.

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------
    float, fallout_rate@k with 3 digits after the decimal point.
    """ 
    # YOUR CODE HERE
    intersec = intersection(true_list,predicted_list[:k])
    # if intersec == []:
    #   return 0.0
    non_relevent_retrived = len(predicted_list[:k]) - len(intersec)
    non_relevent = len(cran_txt_data_titles) - len(true_list)
    return round(non_relevent_retrived /non_relevent ,3)

def f_score(true_list,predicted_list,k=40):
    """
    This function calculate the f_score@k metric.

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------
    float, f-score@k with 3 digits after the decimal point.
    """   
    # YOUR CODE HERE
    P_k = precision_at_k(true_list=true_list, predicted_list=predicted_list, k=k)
    R_k = recall_at_k(true_list=true_list, predicted_list=predicted_list, k=k)
    if P_k == 0 or R_k ==0 :
      return 0.0
    return round( (2*P_k*R_k)/(P_k+R_k) ,3)

def average_precision(true_list,predicted_list,k=40):
    """
    This function calculate the average_precision@k metric.(i.e., precision in every recall point).     

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------
    float, average precision@k with 3 digits after the decimal point.
    """
    # YOUR CODE HERE
    r_k_intersec = intersection(predicted_list[:k],true_list)
    if len(r_k_intersec) == 0:
      return 0.0
    precision = []
    for r in r_k_intersec:
      precision.append(precision_at_k(true_list,predicted_list[:k], predicted_list.index(r)+1))
    ans = sum(precision)/len(precision)
    return round(ans,3)

def ndcg_at_k(true_tuple_list,predicted_list,k=40):
    """
    This function calculate the ndcg@k metric.

    Parameters
    -----------
    true_list: list of relevant documents. Each element is a doc_id.
    predicted_list: sorted list of documents predicted as relevant. Each element is a doc_id. Sorted is performed by relevance score
    k: integer, a number to slice the length of the predicted_list
    
    Returns:
    -----------
    float, ndcg@k with 3 digits after the decimal point.
    """ 
    true_list, relevence_scale = zip(*true_tuple_list)
    predicted_list = intersection(true_list, predicted_list[:k])
    if predicted_list == []:
      return 0

    predicted_list = [relevence_scale[true_list.index(x)] for x in predicted_list]
    actual_DCG=[]
    i = 1
    for rel in predicted_list:
      actual_DCG.append(rel / math.log(i + 1 ,2))
      i += 1
    
    perfect_ranking = sorted(predicted_list, reverse=True)
    ideal_DCG=[]
    j = 1
    for s_rel in perfect_ranking:
      ideal_DCG.append(rel / math.log(j + 1 ,2))
      j += 1
      
    return round((sum(actual_DCG) / sum(ideal_DCG)), 3)

"""Next, we define `evaluate`. A function that calculates multiple metrics and returns a dictionary with metrics scores across different queries."""

def evaluate(true_relevancy,predicted_relevancy,k,print_scores=True):
    """
    This function calculates multiple metrics and returns a dictionary with metrics scores across different queries.
    Parameters
    -----------
    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.
    Example of a single element in the list: 
                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',
                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})
     
    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.
                         The list is sorted by the score.  
    Example:
            key: 1
            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]            
            
    k: integer, a number to slice the length of the predicted_list
    
    print_scores: boolean, enable/disable a print of the mean value of each metric.
    
    Returns:
    -----------
    a dictionary of metrics scores as follows: 
                                                        key: metric name
                                                        value: list of metric scores. Each element corresponds to a given query.
    """    

    recall_lst = []
    precision_lst = []
    f_score_lst = []
    r_precision_lst = []
    reciprocal_rank_lst = []
    avg_precision_lst = []
    fallout_rate_lst = []
    ndcg_lst = []
    metrices = {'recall@k':recall_lst,
                'precision@k':precision_lst,
                'f_score@k': f_score_lst,
                'r-precision': r_precision_lst,
                'MRR@k':reciprocal_rank_lst,
                'MAP@k':avg_precision_lst,
                'fallout@k':fallout_rate_lst,
                'ndcg@k':ndcg_lst}
    
    for query_id, query_info in tqdm(true_relevancy):
        predicted = [doc_id for doc_id, score in predicted_relevancy[query_id]]    
        ground_true = [int(doc_id) for doc_id, score in query_info['relevance_assessments']]
    
        recall_lst.append(recall_at_k(ground_true,predicted,k=k))
        precision_lst.append(precision_at_k(ground_true,predicted,k=k))
        f_score_lst.append(f_score(ground_true,predicted,k=k))
        r_precision_lst.append(r_precision(ground_true,predicted))
        reciprocal_rank_lst.append(reciprocal_rank_at_k(ground_true,predicted,k=k))
        avg_precision_lst.append(average_precision(ground_true,predicted,k=k))
        fallout_rate_lst.append(fallout_rate(ground_true,predicted,k=k))
        ndcg_lst.append(ndcg_at_k(query_info['relevance_assessments'],predicted,k=k))

    if print_scores:
        for name,values in metrices.items():
                print(name,sum(values)/len(values))

    return metrices

"""Let's examine the metrices scores according to the test set"""

# For demonstration we will use N=100. 

N = 100
bm25_queries_score_test_title = bm25_title.search(cran_txt_query_text_test,N=N)
bm25_queries_score_test_body = bm25_body.search(cran_txt_query_text_test,N=N)
half_and_half_test = merge_results(bm25_queries_score_test_title,bm25_queries_score_test_body,N=N)



#tests
ground_true = [int(doc_id) for doc_id, score in cran_qry_rel_data_test[0][1]['relevance_assessments']]
predicted = [doc_id for doc_id, score in half_and_half_test[181]]
assert precision_at_k(ground_true,predicted,k=20) == 0.15
assert recall_at_k(ground_true,predicted,k=20) == 0.5
assert reciprocal_rank_at_k(ground_true,predicted,k=4) == 0.0
assert reciprocal_rank_at_k(ground_true,predicted,k=5) == 0.2
assert r_precision(ground_true,predicted) == 0.333
assert f_score(ground_true,predicted,3) == f_score(ground_true,predicted,4)
assert f_score(ground_true,predicted,4) < f_score(ground_true,predicted,5)
assert ndcg_at_k(cran_qry_rel_data_test[0][1]['relevance_assessments'],predicted,4) == reciprocal_rank_at_k(ground_true,predicted,k=4)
assert ndcg_at_k(cran_qry_rel_data_test[0][1]['relevance_assessments'],predicted,5) > 0
assert fallout_rate(ground_true,predicted,4000) == fallout_rate(ground_true,predicted,40000)
assert fallout_rate(ground_true,predicted,1) == 0.001
assert average_precision(ground_true,predicted,5) == 0.2

ground_true = [int(doc_id) for doc_id, score in cran_qry_rel_data_test[1][1]['relevance_assessments']]
predicted = [doc_id for doc_id, score in half_and_half_test[182]] 
assert precision_at_k(ground_true,predicted,k=4) == 0.5
assert recall_at_k(ground_true,predicted,k=4) == 0.667
assert reciprocal_rank_at_k(ground_true,predicted,k=4) == 1
assert r_precision(ground_true,predicted) == 0.667
assert ndcg_at_k(cran_qry_rel_data_test[1][1]['relevance_assessments'],predicted,16) == 1
assert average_precision(ground_true,predicted,17) == round((precision_at_k(ground_true,predicted,k=1)+precision_at_k(ground_true,predicted,k=2)+precision_at_k(ground_true,predicted,k=17))/3,3)

"""## Task 5: Optimization (5 points)
Optimize your ranking function for MAP@N using the training set.
Try 2 different (not default) combinations of the free parameters and try them with 2 different weight (not included equal weights) on the merge of title and body indecis. 

**Using the training set only.**

Than, choose you final model and evalute it using MAP@30 on the test set.

**YOUR TASK (5 POINTS):** Complete the implementation of the `grid_search_models`
"""

def grid_search_models(data,true_relevancy,bm25_param_list,w_list,N,idx_title,idx_body):

    """
    This function is performing a grid search upon different combination of parameters.
    The parameters can be BM25 parameters (i.e., bm25_param_list) or different weights (i.e., w_list).
    
    Parameters
    ----------
    data: dictionary as follows: 
                            key: query_id
                            value: list of tokens

    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.
    Example of a single element in the list: 
                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',
                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})     
        
    bm25_param_list: list of tuples. Each tuple represent (k,b1) values.

    w_list: list of tuples. Each tuple represent (title_weight,body_weight).    
    N: Integer. How many document to retrieve. 
    
    idx_title: index build upon titles
    idx_body:  index build upon bodies
    ----------
    return: dictionary as follows:
                            key: tuple indiciating the parameters examined in the model (k1,b,title_weight,body_weight)
                            value: MAP@N score
    """
    models = {}
    # YOUR CODE HERE
    for w in w_list:
      for k,b1, in bm25_param_list:

        bm_25_title = BM25_from_index(idx_title, k, b1)
        bm_25_title_search = bm_25_title.search(data)
        bm_25_body = BM25_from_index(idx_body, k, b1)
        bm_25_body_search = bm_25_body.search(data)
        merged_results = merge_results(bm_25_title_search,bm_25_body_search,title_weight=w[0],text_weight=w[1],N = N)
        ans = evaluate(true_relevancy=true_relevancy,predicted_relevancy=merged_results,k=N,print_scores=True)
        models[(k,b1,w[0],w[1])] = ans['MAP@k']
    return models



parameters_list = [(1.4,0.8),(1.6,0.7)]
weights_list = [(0.35,0.65),(0.2,0.8)]
N = 30
grid_models = grid_search_models(cran_txt_query_text_train,cran_qry_rel_data_train,parameters_list,weights_list,N,idx_title,idx_body)

#tests
# get key for max value in dictioary
k1,b,title_w,body_w = max(grid_models, key=grid_models.get)

bm25_title = BM25_from_index(idx_title,k1=k1,b=b)        
bm25_body = BM25_from_index(idx_body,k1=k1,b=b)
bm25_queries_score_test_title = bm25_title.search(cran_txt_query_text_test,N=N)
bm25_queries_score_test_body = bm25_body.search(cran_txt_query_text_test,N=N)
merge_res = merge_results(bm25_queries_score_test_title,bm25_queries_score_test_body,title_weight=title_w,text_weight=body_w,N=N)
test_metrices = evaluate(cran_qry_rel_data_test,merge_res,k=N,print_scores=False)

assert round(sum(test_metrices['MAP@k'])/len(test_metrices['MAP@k']),3) > 0.52
assert len(test_metrices['MAP@k']) == 45
assert max(test_metrices['MAP@k']) == 1
assert min(test_metrices['MAP@k']) == 0
assert len(grid_models) >= 4

"""## Task 6: Plots (10 points)

**YOUR TASK (5 POINTS):** Complete the implementation of `plot_metric_with_differnt_k_values`. 
Plot different values of precision and recall (utilizing different k values) for a given query
"""

def plot_metric_with_differnt_k_values(true_relevancy,predicted_relevancy,metrices_names,k_values):    
    """
    This function plot a for each given metric its value depands on k_values as line chart.
    This function does not return any value.

    Parameters
    ----------
    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.
    Example of a single element in the list: 
                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',
                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})

    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.
                         The list is sorted by the score.  
    Example:
            key: 1
            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]

    metrices_names: list of string representing the metrices to plot. For example: ['precision@k','recall@k','f_score@k']

    k_values: list of integer of different k values. For example [1,3,5]

    returns: 
    plot values in format : 
    
    statistics[metric_name] = [values , k_values]

    """
    statistics = {}
    x_queries = len(list(predicted_relevancy.keys()))
    plt.xlabel('Queries')
    plt.ylabel('metric rate')
    for metric_name in metrices_names:
    # YOUR CODE HERE
        values=[]
        for k in k_values:
            evaluate_dict = evaluate(true_relevancy,predicted_relevancy,k)
            values.append(sum(evaluate_dict[metric_name])*(1/x_queries))
        statistics[metric_name] = [values,k_values]
        plt.title(metric_name)
        plt.plot(statistics[metric_name][1], statistics[metric_name][0], label=metric_name)
        plt.show()

    return statistics

"""**YOUR TASK (5 POINTS):** Complete the implementation of `plot_metric_different_quieries`. 
Plot different values of metrices (utilizing same k values) for different queries.
"""

def plot_metric_different_quieries(true_relevancy,predicted_relevancy,metrices_names,k):
    """
    This function plot for each given metric its value across all queries.
    This function does not return any value.

    Parameters
    ----------
    true_relevancy: list of tuples indicating the relevancy score for a query. Each element corresponds to a query.
    Example of a single element in the list: 
                                            (3, {'question': ' what problems of heat conduction in composite slabs have been solved so far . ',
                                            'relevance_assessments': [(5, 3), (6, 3), (90, 3), (91, 3), (119, 3), (144, 3), (181, 3), (399, 3), (485, 1)]})

    predicted_relevancy: a dictionary of the list. Each key represents the query_id. The value of the dictionary is a sorted list of relevant documents and their scores.
                         The list is sorted by the score.  
    Example:
            key: 1
            value: [(13, 17.256625), (486, 13.539465), (12, 9.957595), (746, 9.599499999999999), (51, 9.171265), .....]

    metrices_names: list of string representing the metrices to plot. For example: ['precision@k','recall@k','f_score@k']
    k: integer, a number to slice the length of the predicted_list.
    
    """
    x_queries = [i for i in (list(predicted_relevancy.keys()))]
    # x_queries = len(list(predicted_relevancy.keys()))
    plt.xlabel('Queries')
    plt.ylabel('metric rate')
    evaluate_dict = evaluate(true_relevancy,predicted_relevancy,k)
    for metric_name in metrices_names:
        y_metric_rate = evaluate_dict[metric_name]
        # print(y_metric_rate)
        plt.title(metric_name)
        plt.plot(x_queries, y_metric_rate, label=metric_name)
        plt.show()
    # plt.legend()
    # plt.show()

k_values = [1,3,5,10,20,30,40]
metrices_names = ['precision@k','recall@k','f_score@k','MRR@k','fallout@k']
statistics_of_differnt_k = plot_metric_with_differnt_k_values(cran_qry_rel_data_test,merge_res,metrices_names,k_values=k_values)

assert  all(statistics_of_differnt_k['precision@k'][0][i-1] > statistics_of_differnt_k['precision@k'][0][i] for i in range(1, len(statistics_of_differnt_k['precision@k'])))
assert  all(statistics_of_differnt_k['recall@k'][0][i-1] < statistics_of_differnt_k['recall@k'][0][i] for i in range(1, len(statistics_of_differnt_k['recall@k'])))
assert  all(statistics_of_differnt_k['MRR@k'][0][i-1] <= statistics_of_differnt_k['MRR@k'][0][i] for i in range(1, len(statistics_of_differnt_k['MRR@k'])))
assert  all(statistics_of_differnt_k['fallout@k'][0][i-1] <= statistics_of_differnt_k['fallout@k'][0][i] for i in range(1, len(statistics_of_differnt_k['fallout@k'])))

print(len(cran_qry_rel_data_test))
plot_metric_different_quieries(cran_qry_rel_data_test,merge_res,metrices_names,k=N)

assert True